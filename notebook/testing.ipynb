{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9eef3d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_cpp import  Llama\n",
    "import tempfile\n",
    "import git\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4974a9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 39 key-value pairs and 288 tensors from ../models/gemma-2b-it.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = gemma2\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Gemma 2 2b It\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = it\n",
      "llama_model_loader: - kv   4:                           general.basename str              = gemma-2\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 2B\n",
      "llama_model_loader: - kv   6:                            general.license str              = gemma\n",
      "llama_model_loader: - kv   7:                               general.tags arr[str,2]       = [\"conversational\", \"text-generation\"]\n",
      "llama_model_loader: - kv   8:                      gemma2.context_length u32              = 8192\n",
      "llama_model_loader: - kv   9:                    gemma2.embedding_length u32              = 2304\n",
      "llama_model_loader: - kv  10:                         gemma2.block_count u32              = 26\n",
      "llama_model_loader: - kv  11:                 gemma2.feed_forward_length u32              = 9216\n",
      "llama_model_loader: - kv  12:                gemma2.attention.head_count u32              = 8\n",
      "llama_model_loader: - kv  13:             gemma2.attention.head_count_kv u32              = 4\n",
      "llama_model_loader: - kv  14:    gemma2.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  15:                gemma2.attention.key_length u32              = 256\n",
      "llama_model_loader: - kv  16:              gemma2.attention.value_length u32              = 256\n",
      "llama_model_loader: - kv  17:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  18:              gemma2.attn_logit_softcapping f32              = 50.000000\n",
      "llama_model_loader: - kv  19:             gemma2.final_logit_softcapping f32              = 30.000000\n",
      "llama_model_loader: - kv  20:            gemma2.attention.sliding_window u32              = 4096\n",
      "llama_model_loader: - kv  21:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  22:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  23:                      tokenizer.ggml.tokens arr[str,256000]  = [\"<pad>\", \"<eos>\", \"<bos>\", \"<unk>\", ...\n",
      "llama_model_loader: - kv  24:                      tokenizer.ggml.scores arr[f32,256000]  = [-1000.000000, -1000.000000, -1000.00...\n",
      "llama_model_loader: - kv  25:                  tokenizer.ggml.token_type arr[i32,256000]  = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...\n",
      "llama_model_loader: - kv  26:                tokenizer.ggml.bos_token_id u32              = 2\n",
      "llama_model_loader: - kv  27:                tokenizer.ggml.eos_token_id u32              = 1\n",
      "llama_model_loader: - kv  28:            tokenizer.ggml.unknown_token_id u32              = 3\n",
      "llama_model_loader: - kv  29:            tokenizer.ggml.padding_token_id u32              = 0\n",
      "llama_model_loader: - kv  30:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  31:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  32:                    tokenizer.chat_template str              = {{ bos_token }}{% if messages[0]['rol...\n",
      "llama_model_loader: - kv  33:            tokenizer.ggml.add_space_prefix bool             = false\n",
      "llama_model_loader: - kv  34:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  35:                      quantize.imatrix.file str              = /models_out/gemma-2-2b-it-GGUF/gemma-...\n",
      "llama_model_loader: - kv  36:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt\n",
      "llama_model_loader: - kv  37:             quantize.imatrix.entries_count i32              = 182\n",
      "llama_model_loader: - kv  38:              quantize.imatrix.chunks_count i32              = 128\n",
      "llama_model_loader: - type  f32:  105 tensors\n",
      "llama_model_loader: - type q4_K:  156 tensors\n",
      "llama_model_loader: - type q6_K:   27 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 1.59 GiB (5.21 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control token:      1 '<eos>' is not marked as EOG\n",
      "load: control token:      0 '<pad>' is not marked as EOG\n",
      "load: control token:     47 '<unused40>' is not marked as EOG\n",
      "load: control token:      3 '<unk>' is not marked as EOG\n",
      "load: control token:     55 '<unused48>' is not marked as EOG\n",
      "load: control token:     24 '<unused17>' is not marked as EOG\n",
      "load: control token:      2 '<bos>' is not marked as EOG\n",
      "load: control token:      5 '<2mass>' is not marked as EOG\n",
      "load: control token:     58 '<unused51>' is not marked as EOG\n",
      "load: control token:      4 '<mask>' is not marked as EOG\n",
      "load: control token:     42 '<unused35>' is not marked as EOG\n",
      "load: control token:      9 '<unused2>' is not marked as EOG\n",
      "load: control token:      6 '[@BOS@]' is not marked as EOG\n",
      "load: control token:     44 '<unused37>' is not marked as EOG\n",
      "load: control token:     35 '<unused28>' is not marked as EOG\n",
      "load: control token:      7 '<unused0>' is not marked as EOG\n",
      "load: control token:     43 '<unused36>' is not marked as EOG\n",
      "load: control token:     36 '<unused29>' is not marked as EOG\n",
      "load: control token:      8 '<unused1>' is not marked as EOG\n",
      "load: control token:     41 '<unused34>' is not marked as EOG\n",
      "load: control token:     10 '<unused3>' is not marked as EOG\n",
      "load: control token:     40 '<unused33>' is not marked as EOG\n",
      "load: control token:     11 '<unused4>' is not marked as EOG\n",
      "load: control token:     39 '<unused32>' is not marked as EOG\n",
      "load: control token:     12 '<unused5>' is not marked as EOG\n",
      "load: control token:     38 '<unused31>' is not marked as EOG\n",
      "load: control token:     13 '<unused6>' is not marked as EOG\n",
      "load: control token:     37 '<unused30>' is not marked as EOG\n",
      "load: control token:     14 '<unused7>' is not marked as EOG\n",
      "load: control token:     27 '<unused20>' is not marked as EOG\n",
      "load: control token:     15 '<unused8>' is not marked as EOG\n",
      "load: control token:     28 '<unused21>' is not marked as EOG\n",
      "load: control token:     16 '<unused9>' is not marked as EOG\n",
      "load: control token:     17 '<unused10>' is not marked as EOG\n",
      "load: control token:     18 '<unused11>' is not marked as EOG\n",
      "load: control token:     19 '<unused12>' is not marked as EOG\n",
      "load: control token:     20 '<unused13>' is not marked as EOG\n",
      "load: control token:     21 '<unused14>' is not marked as EOG\n",
      "load: control token:     22 '<unused15>' is not marked as EOG\n",
      "load: control token:     56 '<unused49>' is not marked as EOG\n",
      "load: control token:     23 '<unused16>' is not marked as EOG\n",
      "load: control token:     54 '<unused47>' is not marked as EOG\n",
      "load: control token:     25 '<unused18>' is not marked as EOG\n",
      "load: control token:     53 '<unused46>' is not marked as EOG\n",
      "load: control token:     26 '<unused19>' is not marked as EOG\n",
      "load: control token:     29 '<unused22>' is not marked as EOG\n",
      "load: control token:     30 '<unused23>' is not marked as EOG\n",
      "load: control token:     31 '<unused24>' is not marked as EOG\n",
      "load: control token:     32 '<unused25>' is not marked as EOG\n",
      "load: control token:     46 '<unused39>' is not marked as EOG\n",
      "load: control token:     33 '<unused26>' is not marked as EOG\n",
      "load: control token:     45 '<unused38>' is not marked as EOG\n",
      "load: control token:     34 '<unused27>' is not marked as EOG\n",
      "load: control token:     48 '<unused41>' is not marked as EOG\n",
      "load: control token:     49 '<unused42>' is not marked as EOG\n",
      "load: control token:     50 '<unused43>' is not marked as EOG\n",
      "load: control token:     51 '<unused44>' is not marked as EOG\n",
      "load: control token:     52 '<unused45>' is not marked as EOG\n",
      "load: control token:     57 '<unused50>' is not marked as EOG\n",
      "load: control token:     59 '<unused52>' is not marked as EOG\n",
      "load: control token:     60 '<unused53>' is not marked as EOG\n",
      "load: control token:     61 '<unused54>' is not marked as EOG\n",
      "load: control token:     62 '<unused55>' is not marked as EOG\n",
      "load: control token:     63 '<unused56>' is not marked as EOG\n",
      "load: control token:     64 '<unused57>' is not marked as EOG\n",
      "load: control token:     65 '<unused58>' is not marked as EOG\n",
      "load: control token:     66 '<unused59>' is not marked as EOG\n",
      "load: control token:     67 '<unused60>' is not marked as EOG\n",
      "load: control token:     68 '<unused61>' is not marked as EOG\n",
      "load: control token:     69 '<unused62>' is not marked as EOG\n",
      "load: control token:     70 '<unused63>' is not marked as EOG\n",
      "load: control token:     71 '<unused64>' is not marked as EOG\n",
      "load: control token:     72 '<unused65>' is not marked as EOG\n",
      "load: control token:     73 '<unused66>' is not marked as EOG\n",
      "load: control token:     74 '<unused67>' is not marked as EOG\n",
      "load: control token:     75 '<unused68>' is not marked as EOG\n",
      "load: control token:     76 '<unused69>' is not marked as EOG\n",
      "load: control token:     77 '<unused70>' is not marked as EOG\n",
      "load: control token:     78 '<unused71>' is not marked as EOG\n",
      "load: control token:     79 '<unused72>' is not marked as EOG\n",
      "load: control token:     80 '<unused73>' is not marked as EOG\n",
      "load: control token:     81 '<unused74>' is not marked as EOG\n",
      "load: control token:     82 '<unused75>' is not marked as EOG\n",
      "load: control token:     83 '<unused76>' is not marked as EOG\n",
      "load: control token:     84 '<unused77>' is not marked as EOG\n",
      "load: control token:     85 '<unused78>' is not marked as EOG\n",
      "load: control token:     86 '<unused79>' is not marked as EOG\n",
      "load: control token:     87 '<unused80>' is not marked as EOG\n",
      "load: control token:     88 '<unused81>' is not marked as EOG\n",
      "load: control token:     89 '<unused82>' is not marked as EOG\n",
      "load: control token:     90 '<unused83>' is not marked as EOG\n",
      "load: control token:     91 '<unused84>' is not marked as EOG\n",
      "load: control token:     92 '<unused85>' is not marked as EOG\n",
      "load: control token:     93 '<unused86>' is not marked as EOG\n",
      "load: control token:     94 '<unused87>' is not marked as EOG\n",
      "load: control token:     95 '<unused88>' is not marked as EOG\n",
      "load: control token:     96 '<unused89>' is not marked as EOG\n",
      "load: control token:     97 '<unused90>' is not marked as EOG\n",
      "load: control token:     98 '<unused91>' is not marked as EOG\n",
      "load: control token:     99 '<unused92>' is not marked as EOG\n",
      "load: control token:    100 '<unused93>' is not marked as EOG\n",
      "load: control token:    101 '<unused94>' is not marked as EOG\n",
      "load: control token:    102 '<unused95>' is not marked as EOG\n",
      "load: control token:    103 '<unused96>' is not marked as EOG\n",
      "load: control token:    104 '<unused97>' is not marked as EOG\n",
      "load: control token:    105 '<unused98>' is not marked as EOG\n",
      "load: control token:    106 '<start_of_turn>' is not marked as EOG\n",
      "load: control token: 255999 '<unused99>' is not marked as EOG\n",
      "load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n",
      "load: special tokens cache size = 249\n",
      "load: token to piece cache size = 1.6014 MB\n",
      "print_info: arch             = gemma2\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 8192\n",
      "print_info: n_embd           = 2304\n",
      "print_info: n_layer          = 26\n",
      "print_info: n_head           = 8\n",
      "print_info: n_head_kv        = 4\n",
      "print_info: n_rot            = 256\n",
      "print_info: n_swa            = 4096\n",
      "print_info: is_swa_any       = 1\n",
      "print_info: n_embd_head_k    = 256\n",
      "print_info: n_embd_head_v    = 256\n",
      "print_info: n_gqa            = 2\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-06\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 6.2e-02\n",
      "print_info: n_ff             = 9216\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 2\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 8192\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: model type       = 2B\n",
      "print_info: model params     = 2.61 B\n",
      "print_info: general.name     = Gemma 2 2b It\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 256000\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 2 '<bos>'\n",
      "print_info: EOS token        = 1 '<eos>'\n",
      "print_info: EOT token        = 107 '<end_of_turn>'\n",
      "print_info: UNK token        = 3 '<unk>'\n",
      "print_info: PAD token        = 0 '<pad>'\n",
      "print_info: LF token         = 227 '<0x0A>'\n",
      "print_info: EOG token        = 1 '<eos>'\n",
      "print_info: EOG token        = 107 '<end_of_turn>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer   1 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   2 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer   3 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   4 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer   5 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   6 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer   7 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer   8 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer   9 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  10 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  11 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  12 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  13 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  14 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  15 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  16 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  17 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  18 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  19 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  20 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  21 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  22 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  23 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  24 assigned to device CPU, is_swa = 1\n",
      "load_tensors: layer  25 assigned to device CPU, is_swa = 0\n",
      "load_tensors: layer  26 assigned to device CPU, is_swa = 0\n",
      "load_tensors: tensor 'token_embd.weight' (q6_K) (and 132 others) cannot be used with preferred buffer type CPU_REPACK, using CPU instead\n",
      "load_tensors:   CPU_REPACK model buffer size =   921.38 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =  1623.67 MiB\n",
      "repack: repack tensor blk.0.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.0.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.0.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.0.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.0.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.1.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.1.attn_k.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.1.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.1.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.1.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.2.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.2.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.2.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.2.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.2.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.2.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.2.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.3.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.3.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.3.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.4.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.4.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.4.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.5.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.5.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.5.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.5.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.6.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.6.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.6.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.7.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.7.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.7.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.8.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.8.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.8.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.9.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.9.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.9.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.10.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.10.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.10.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.attn_k.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.11.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.11.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.11.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.12.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.12.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.12.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.12.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.13.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.13.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.13.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.attn_k.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.14.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.14.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.14.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.15.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.15.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.15.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.15.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.16.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.16.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.16.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.attn_v.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.17.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.17.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.17.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.18.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.18.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.18.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.18.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.18.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.18.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.18.ffn_down.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.19.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.19.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.20.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.20.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.20.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.20.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.21.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.21.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.21.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.21.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.21.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.22.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.22.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.22.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.23.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.attn_v.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.attn_output.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.23.ffn_gate.weight with q4_K_8x8\n",
      "repack: repack tensor blk.23.ffn_up.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.23.ffn_down.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.24.attn_q.weight with q4_K_8x8\n",
      "repack: repack tensor blk.24.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.24.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.24.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.24.ffn_up.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.attn_q.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.25.attn_k.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.attn_output.weight with q4_K_8x8\n",
      "repack: repack tensor blk.25.ffn_gate.weight with q4_K_8x8\n",
      ".repack: repack tensor blk.25.ffn_up.weight with q4_K_8x8\n",
      "...............\n",
      "llama_context: constructing llama_context\n",
      "llama_context: n_seq_max     = 1\n",
      "llama_context: n_ctx         = 12000\n",
      "llama_context: n_ctx_per_seq = 12000\n",
      "llama_context: n_batch       = 512\n",
      "llama_context: n_ubatch      = 512\n",
      "llama_context: causal_attn   = 1\n",
      "llama_context: flash_attn    = 0\n",
      "llama_context: freq_base     = 10000.0\n",
      "llama_context: freq_scale    = 1\n",
      "llama_context: n_ctx_per_seq (12000) > n_ctx_train (8192) -- possible training context overflow\n",
      "set_abort_callback: call\n",
      "llama_context:        CPU  output buffer size =     0.98 MiB\n",
      "create_memory: n_ctx = 12000 (padded)\n",
      "llama_kv_cache_unified_iswa: using full-size SWA cache (ref: https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)\n",
      "llama_kv_cache_unified_iswa: creating non-SWA KV cache, size = 12000 cells\n",
      "llama_kv_cache_unified: layer   0: skipped\n",
      "llama_kv_cache_unified: layer   1: dev = CPU\n",
      "llama_kv_cache_unified: layer   2: skipped\n",
      "llama_kv_cache_unified: layer   3: dev = CPU\n",
      "llama_kv_cache_unified: layer   4: skipped\n",
      "llama_kv_cache_unified: layer   5: dev = CPU\n",
      "llama_kv_cache_unified: layer   6: skipped\n",
      "llama_kv_cache_unified: layer   7: dev = CPU\n",
      "llama_kv_cache_unified: layer   8: skipped\n",
      "llama_kv_cache_unified: layer   9: dev = CPU\n",
      "llama_kv_cache_unified: layer  10: skipped\n",
      "llama_kv_cache_unified: layer  11: dev = CPU\n",
      "llama_kv_cache_unified: layer  12: skipped\n",
      "llama_kv_cache_unified: layer  13: dev = CPU\n",
      "llama_kv_cache_unified: layer  14: skipped\n",
      "llama_kv_cache_unified: layer  15: dev = CPU\n",
      "llama_kv_cache_unified: layer  16: skipped\n",
      "llama_kv_cache_unified: layer  17: dev = CPU\n",
      "llama_kv_cache_unified: layer  18: skipped\n",
      "llama_kv_cache_unified: layer  19: dev = CPU\n",
      "llama_kv_cache_unified: layer  20: skipped\n",
      "llama_kv_cache_unified: layer  21: dev = CPU\n",
      "llama_kv_cache_unified: layer  22: skipped\n",
      "llama_kv_cache_unified: layer  23: dev = CPU\n",
      "llama_kv_cache_unified: layer  24: skipped\n",
      "llama_kv_cache_unified: layer  25: dev = CPU\n",
      "llama_kv_cache_unified:        CPU KV buffer size =   609.38 MiB\n",
      "llama_kv_cache_unified: size =  609.38 MiB ( 12000 cells,  13 layers,  1 seqs), K (f16):  304.69 MiB, V (f16):  304.69 MiB\n",
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n",
      "llama_kv_cache_unified_iswa: creating     SWA KV cache, size = 12000 cells\n",
      "llama_kv_cache_unified: layer   0: dev = CPU\n",
      "llama_kv_cache_unified: layer   1: skipped\n",
      "llama_kv_cache_unified: layer   2: dev = CPU\n",
      "llama_kv_cache_unified: layer   3: skipped\n",
      "llama_kv_cache_unified: layer   4: dev = CPU\n",
      "llama_kv_cache_unified: layer   5: skipped\n",
      "llama_kv_cache_unified: layer   6: dev = CPU\n",
      "llama_kv_cache_unified: layer   7: skipped\n",
      "llama_kv_cache_unified: layer   8: dev = CPU\n",
      "llama_kv_cache_unified: layer   9: skipped\n",
      "llama_kv_cache_unified: layer  10: dev = CPU\n",
      "llama_kv_cache_unified: layer  11: skipped\n",
      "llama_kv_cache_unified: layer  12: dev = CPU\n",
      "llama_kv_cache_unified: layer  13: skipped\n",
      "llama_kv_cache_unified: layer  14: dev = CPU\n",
      "llama_kv_cache_unified: layer  15: skipped\n",
      "llama_kv_cache_unified: layer  16: dev = CPU\n",
      "llama_kv_cache_unified: layer  17: skipped\n",
      "llama_kv_cache_unified: layer  18: dev = CPU\n",
      "llama_kv_cache_unified: layer  19: skipped\n",
      "llama_kv_cache_unified: layer  20: dev = CPU\n",
      "llama_kv_cache_unified: layer  21: skipped\n",
      "llama_kv_cache_unified: layer  22: dev = CPU\n",
      "llama_kv_cache_unified: layer  23: skipped\n",
      "llama_kv_cache_unified: layer  24: dev = CPU\n",
      "llama_kv_cache_unified: layer  25: skipped\n",
      "llama_kv_cache_unified:        CPU KV buffer size =   609.38 MiB\n",
      "llama_kv_cache_unified: size =  609.38 MiB ( 12000 cells,  13 layers,  1 seqs), K (f16):  304.69 MiB, V (f16):  304.69 MiB\n",
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n",
      "llama_context: enumerating backends\n",
      "llama_context: backend_ptrs.size() = 1\n",
      "llama_context: max_nodes = 65536\n",
      "llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =    1, n_seqs =  1, n_outputs =    1\n",
      "graph_reserve: reserving a graph for ubatch with n_tokens =  512, n_seqs =  1, n_outputs =  512\n",
      "llama_context:        CPU compute buffer size =   509.00 MiB\n",
      "llama_context: graph nodes  = 1154\n",
      "llama_context: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | REPACK = 1 | \n",
      "Model metadata: {'general.name': 'Gemma 2 2b It', 'general.architecture': 'gemma2', 'general.type': 'model', 'general.basename': 'gemma-2', 'general.finetune': 'it', 'general.size_label': '2B', 'gemma2.context_length': '8192', 'general.license': 'gemma', 'gemma2.embedding_length': '2304', 'gemma2.block_count': '26', 'gemma2.feed_forward_length': '9216', 'gemma2.attention.head_count': '8', 'gemma2.attention.head_count_kv': '4', 'gemma2.attention.layer_norm_rms_epsilon': '0.000001', 'gemma2.attention.key_length': '256', 'gemma2.attention.value_length': '256', 'tokenizer.ggml.eos_token_id': '1', 'gemma2.attn_logit_softcapping': '50.000000', 'general.file_type': '15', 'gemma2.final_logit_softcapping': '30.000000', 'gemma2.attention.sliding_window': '4096', 'tokenizer.ggml.model': 'llama', 'tokenizer.ggml.add_space_prefix': 'false', 'tokenizer.ggml.pre': 'default', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '2', 'tokenizer.ggml.unknown_token_id': '3', 'tokenizer.ggml.padding_token_id': '0', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.chat_template': \"{{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\\n' + message['content'] | trim + '<end_of_turn>\\n' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\\n'}}{% endif %}\", 'quantize.imatrix.chunks_count': '128', 'quantize.imatrix.file': '/models_out/gemma-2-2b-it-GGUF/gemma-2-2b-it.imatrix', 'quantize.imatrix.dataset': '/training_dir/calibration_datav3.txt', 'quantize.imatrix.entries_count': '182'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{ bos_token }}{% if messages[0]['role'] == 'system' %}{{ raise_exception('System role not supported') }}{% endif %}{% for message in messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if (message['role'] == 'assistant') %}{% set role = 'model' %}{% else %}{% set role = message['role'] %}{% endif %}{{ '<start_of_turn>' + role + '\n",
      "' + message['content'] | trim + '<end_of_turn>\n",
      "' }}{% endfor %}{% if add_generation_prompt %}{{'<start_of_turn>model\n",
      "'}}{% endif %}\n",
      "Using chat eos_token: <eos>\n",
      "Using chat bos_token: <bos>\n"
     ]
    }
   ],
   "source": [
    "llm = Llama('../models/gemma-2b-it.Q4_K_M.gguf',n_ctx=12000 , n_threads=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f689f299",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_url = \"https://github.com/mahmuuudtolba/AI-Agent-accent-classifier\"\n",
    "INCLUDE_EXTENSIONS = {\".py\", \".md\", \".yml\", \".yaml\", \".txt\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "efc58712",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tempfile.TemporaryDirectory() as tmpdir:\n",
    "    git.Repo.clone_from(repo_url, tmpdir)\n",
    "\n",
    "    content = []\n",
    "    for root, _, files in os.walk(tmpdir):\n",
    "        for file in files:\n",
    "            if any(file.endswith(ext) for ext in INCLUDE_EXTENSIONS):\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "                    file_content = f.read()\n",
    "                    content.append(f\"\\n\\n### {file} ###\\n{file_content}\")\n",
    "\n",
    "repo_text = \"\\n\".join(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6677295e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12477"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(repo_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1b9da362",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = f\"\"\"<bos><start_of_turn>user\n",
    "                    You are a senior software engineer and AI code reviewer.\n",
    "\n",
    "                    Please review the following GitHub repository and provide a human-readable analysis. Focus on:\n",
    "\n",
    "                    1. **Strengths** – What is done well in terms of code structure, tools used, ML practices, etc.\n",
    "                    2. **Areas for Improvement** – Where the code could be improved (preprocessing, evaluation, structure, etc.)\n",
    "                    3. **File-specific Feedback** – Comments on key files like `data_processing.py`, `model_training.py`, etc.\n",
    "                    4. **Suggestions with Code Snippets** – Short examples or code fragments showing how to improve.\n",
    "                    5. **Optional**: Data visualization or debugging suggestions.\n",
    "\n",
    "                    Keep the tone constructive and technical. Output should be structured using Markdown-style sections.\n",
    "\n",
    "                    Repository Code:\n",
    "                    {repo_text}\n",
    "                    <end_of_turn>\n",
    "                    <start_of_turn>model\n",
    "\n",
    "                    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2059d483",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 182 prefix-match hit, remaining 3451 prompt tokens to eval\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##  Review and Feedback\n",
      "\n",
      "Here's a detailed breakdown of the code, focusing on strengths, weaknesses, and recommendations:\n",
      "\n",
      "**Strengths:**\n",
      "\n",
      "* **Clear Structure:** The repository is well-structured.  It uses a clear file structure for different components, including the main entry point (`app.py`), API endpoints, and data processing/model loading logic.\n",
      "* **REST API Endpoints:** You've defined REST API endpoints for health checks and accent classification. This is a good starting point for a real-world application. \n",
      "* **Modular Approach:** The separation of concerns is good. You've used FastAPI for the API, making it easier to extend and maintain. \n",
      "* **Dockerized:**  Dockerization is a great practice, especially when working with a cloud-based deployment.\n",
      "* **Data Preprocessing:** The code includes basic steps to handle audio files, potentially extracting audio features and preprocessing them for the pretrained model.\n",
      "\n",
      "**Areas for Improvement:**\n",
      "\n",
      "**1. Code Quality and Readability:**\n",
      "\n",
      "* **Type hinting:**  Use type hinting for functions and classes to improve clarity and prevent type errors.  \n",
      "* **Comments:** Add more comments, especially in places where the logic is more complex. Explain the purpose of sections, variables, and how functions work.\n",
      "* **Indentation:** Maintain consistent indentation.\n",
      "\n",
      "**2. Data Handling and Evaluation:**\n",
      "\n",
      "* **Data Validation:** The model is likely to be sensitive to inputs. Add input validation to prevent invalid data from being processed.  \n",
      "* **Model Evaluation:**  Include metrics to measure the model's performance.  Consider using libraries like `scikit-learn` for this.\n",
      "* **Error Handling:** Improve error handling.  Catch potential exceptions early and provide helpful user messages.\n",
      "\n",
      "**3. Model Management:**\n",
      "\n",
      "* **Model Loading:**  It's unclear how the model is loaded. You need to document how the pretrained model is downloaded, loaded, and used for inference. \n",
      "* **Model Saving:** Provide guidance on how to save the trained model.\n",
      "\n",
      "**4. Preprocessing:**\n",
      "\n",
      "* **Feature Extraction:** The code is very basic. Consider using more sophisticated feature extraction techniques (e.g., Mel spectrogram, MFCCs) to improve model performance.\n",
      "* **Data Preprocessing:** You could add steps to normalize audio data or remove noise.\n",
      "\n",
      "\n",
      "**5. Visualization and Debugging:**\n",
      "\n",
      "* **Data Visualizations:** Consider using libraries like `matplotlib` to visualize the data. For example, visualize the audio spectrograms for comparison or the model's predictions.\n",
      "* **Debugging:** Include more detailed error messages with stack traces for debugging purposes.\n",
      "* **Logging:** Add more informative logs to help diagnose potential issues.\n",
      "\n",
      "**Suggestions with Code Snippets:**\n",
      "\n",
      "**1. Type Hinting (Example)**\n",
      "\n",
      "```python\n",
      "from fastapi import FastAPI\n",
      "from typing import List, Dict\n",
      "from pydantic import BaseModel\n",
      " \n",
      "app = FastAPI()\n",
      "\n",
      "class DataInput(BaseModel):\n",
      "    video_url: List[str]\n",
      "\n",
      "\n",
      "@app.get(\"/api/v1/accent_classification\")\n",
      "async def accent_classification(data: DataInput):\n",
      "    start = time.time()\n",
      "    # ... (rest of the code)\n",
      "```\n",
      "\n",
      "**2. Model Evaluation (Example)**\n",
      "\n",
      "```python\n",
      "import sklearn\n",
      "from sklearn.metrics import confusion_matrix\n",
      "\n",
      "def evaluate_model(predictions, true_labels):\n",
      "    confusion_matrix = confusion_matrix(true_labels, predictions)\n",
      "    accuracy = confusion_matrix.diagonal().sum() / (confusion_matrix.sum())\n",
      "    print(f\"Accuracy: {accuracy}\")\n",
      "    return accuracy\n",
      "```\n",
      "\n",
      "\n",
      "**3. Data Validation (Example)**\n",
      "\n",
      "```python\n",
      "from pydantic import BaseModel \n",
      "\n",
      "class DataInput(BaseModel):\n",
      "    video_url: List[str]\n",
      "    \n",
      "    # Add validation for video URLs\n",
      "    @validate(\n",
      "        validator=\"is_valid_url\",\n",
      "        message=\"Invalid video URL, please use a valid URL format.\"\n",
      "    )\n",
      "    def is_valid_url(cls, value):\n",
      "        try: \n",
      "            import requests  \n",
      "            return requests.head(value).status_code == 200\n",
      "        except Exception:\n",
      "            return False \n",
      "\n",
      "\n",
      "# ... (rest of the code) \n",
      "```\n",
      "\n",
      "\n",
      "**Data Visualization (Example with Matplotlib)**\n",
      "\n",
      "```python\n",
      "import matplotlib.pyplot as plt\n",
      "import librosa\n",
      "\n",
      "def plot_audio(audio_data):\n",
      "    # ... (Code to extract audio features, compute spectrograms)\n",
      "    plt.figure(figsize=(12, 4))\n",
      "    librosa.display.specshow(spectrogram, sr=audio_data.sr)\n",
      "    plt.colorbar(format=\"%+2.0f dB\")\n",
      "    plt.title('Spectrogram')\n",
      "    plt.show()\n",
      "\n",
      "```\n",
      "\n",
      " **Additional Recommendations**\n",
      "* **Deployment:** Explore containerization (Docker) and Kubernetes for deployment.\n",
      "* **Security:**  Consider adding security measures for your API (authentication, authorization).\n",
      "* **Testing:** Implement thorough unit and integration tests to ensure reliability.\n",
      "* **Documentation:** Create comprehensive documentation (using tools like Swagger or Doxygen) to help users understand your API. \n",
      "\n",
      "\n",
      "\n",
      " Let me know if you have any more questions. "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  100371.49 ms\n",
      "llama_perf_context_print: prompt eval time =   52859.96 ms /  3452 tokens (   15.31 ms per token,    65.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =  122928.23 ms /  1115 runs   (  110.25 ms per token,     9.07 tokens per second)\n",
      "llama_perf_context_print:       total time =  158482.79 ms /  4567 tokens\n"
     ]
    }
   ],
   "source": [
    "stream = llm.create_completion(prompt , max_tokens=5000 , stream=True)\n",
    "for chunk in stream:\n",
    "    print(chunk[\"choices\"][0][\"text\"] , end=\"\" , flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1af0306",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Analysis of the repository:\n",
      "\n",
      "**Strengths:**\n",
      "\n",
      "* **Clear and Concise README:** The README provides a comprehensive overview of the project, including its purpose, features, technologies used, installation instructions, and usage examples. It's well-structured and easy to understand.\n",
      "* **Dockerization:**  The use of Docker for deployment is excellent practice. It makes deploying and scaling the service much easier and ensures consistent environments.\n",
      "* **API Design:** The RESTful API design is appropriate for an application with this kind of functionality. Using endpoints for health checks and accent classification, and clear data validation in the `DataInput` class makes it maintainable and understandable.\n",
      "* **Data Preprocessing:** The use of the `AudioSegment` library for audio manipulation suggests a well-thought-out pipeline. \n",
      "* **Error Handling:** The code includes basic error handling. It's important to incorporate more comprehensive error handling and exception handling strategies.\n",
      "* **Data Visualization:** While not included in the repo, including visualization (e.g., a graph showing accent prediction scores) could greatly enhance user understanding and provide valuable insights.\n",
      "\n",
      "\n",
      "**Areas for Improvement:**\n",
      "\n",
      "* **ML Practices:** \n",
      "    * **Model Loading:**  The code uses a specific model, but it's unclear if it's fine-tuned or pre-trained for the task. This should be clarified.\n",
      "    * **Model Evaluation:**  The code lacks clear evaluation metrics. You might consider adding metrics like accuracy, precision, recall, and F1 score.  \n",
      "    * **Logging:**  Logging should be expanded to provide detailed information about the model's performance (e.g., training progress, validation performance) and potential issues.\n",
      "    * **Reproducibility:** The code could be made more reproducible by adding detailed documentation for environment setup. \n",
      "\n",
      "* **File-Specific Feedback:**\n",
      "    * **`data_processing.py`:**\n",
      "        * **Data Preprocessing:**  Ensure consistency in the audio format and data preprocessing.  \n",
      "        * **Data Storage:** Consider how the model will handle large datasets. For production-level applications, you'll likely want to use a more robust storage and potentially a database. \n",
      "    * **`model_training.py`:**\n",
      "        * **Fine-tuning:**  If the model is not pretrained, fine-tuning should be explicitly outlined. \n",
      "        * **Hyperparameter Optimization:** Add code for hyperparameter tuning to optimize the model's performance.\n",
      "\n",
      "* **Code Style & Structure:** \n",
      "    * **Encapsulation:**  Consider separating business logic, model management, and data processing into different modules for better organization and code readability.  \n",
      "    * **Comments:** While the code is generally well-commented, more detailed explanations of how certain methods work would be beneficial.\n",
      "    * **Defensive Programming:** Add more robust checks to handle potential input errors (e.g., invalid audio formats, empty data, etc.). \n",
      "    * **Data Validation:** Implement more comprehensive data validation for the input data.\n",
      "    * **Code Formatting:**  Use consistent indentation and whitespace to enhance readability.  \n",
      "\n",
      "* **Data Visualization:** Consider integrating a visualization library (e.g., Plotly) for a more interactive experience and better understanding of results.\n",
      "\n",
      "**Example Snippets and Code Improvements:**\n",
      "\n",
      "\n",
      "* **Improved `accent_classification.py`:**\n",
      "    ```python\n",
      "    import warnings\n",
      "    warnings.filterwarnings('ignore')\n",
      "    \n",
      "    def predict_accent(audio_path):\n",
      "        try:\n",
      "            pipe = pipeline(\"audio-classification\", model=local_model_dir)\n",
      "            # ... (Model inference code here) ... \n",
      "        except Exception as e:\n",
      "            return {\"error\": str(e)}\n",
      "\n",
      "    @app.post(\"/api/v1/accent_classification\") \n",
      "    async def predict_accent_classification(data: DataInput):\n",
      "        # ... (Code to load and process the audio)\n",
      "        result = predict_accent(audio_path)\n",
      "        return result\n",
      "    ```\n",
      "\n",
      "\n",
      "**Additional Recommendations:**\n",
      "\n",
      "* **Testing:** Implement unit tests to ensure individual functions are working as expected.  \n",
      "* **Security:**  Review the security of your API (e.g., input validation, authorization, data protection). \n",
      "\n",
      "\n",
      "By incorporating these suggestions, you can enhance the code's quality, efficiency, and scalability.  Remember that consistent improvement and attention to details are key to a successful project. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"## Analysis of the repository:\\n\\n**Strengths:**\\n\\n* **Clear and Concise README:** The README provides a comprehensive overview of the project, including its purpose, features, technologies used, installation instructions, and usage examples. It's well-structured and easy to understand.\\n* **Dockerization:**  The use of Docker for deployment is excellent practice. It makes deploying and scaling the service much easier and ensures consistent environments.\\n* **API Design:** The RESTful API design is appropriate for an application with this kind of functionality. Using endpoints for health checks and accent classification, and clear data validation in the `DataInput` class makes it maintainable and understandable.\\n* **Data Preprocessing:** The use of the `AudioSegment` library for audio manipulation suggests a well-thought-out pipeline. \\n* **Error Handling:** The code includes basic error handling. It's important to incorporate more comprehensive error handling and exception handling strategies.\\n* **Data Visualization:** While not included in the repo, including visualization (e.g., a graph showing accent prediction scores) could greatly enhance user understanding and provide valuable insights.\\n\\n\\n**Areas for Improvement:**\\n\\n* **ML Practices:** \\n    * **Model Loading:**  The code uses a specific model, but it's unclear if it's fine-tuned or pre-trained for the task. This should be clarified.\\n    * **Model Evaluation:**  The code lacks clear evaluation metrics. You might consider adding metrics like accuracy, precision, recall, and F1 score.  \\n    * **Logging:**  Logging should be expanded to provide detailed information about the model's performance (e.g., training progress, validation performance) and potential issues.\\n    * **Reproducibility:** The code could be made more reproducible by adding detailed documentation for environment setup. \\n\\n* **File-Specific Feedback:**\\n    * **`data_processing.py`:**\\n        * **Data Preprocessing:**  Ensure consistency in the audio format and data preprocessing.  \\n        * **Data Storage:** Consider how the model will handle large datasets. For production-level applications, you'll likely want to use a more robust storage and potentially a database. \\n    * **`model_training.py`:**\\n        * **Fine-tuning:**  If the model is not pretrained, fine-tuning should be explicitly outlined. \\n        * **Hyperparameter Optimization:** Add code for hyperparameter tuning to optimize the model's performance.\\n\\n* **Code Style & Structure:** \\n    * **Encapsulation:**  Consider separating business logic, model management, and data processing into different modules for better organization and code readability.  \\n    * **Comments:** While the code is generally well-commented, more detailed explanations of how certain methods work would be beneficial.\\n    * **Defensive Programming:** Add more robust checks to handle potential input errors (e.g., invalid audio formats, empty data, etc.). \\n    * **Data Validation:** Implement more comprehensive data validation for the input data.\\n    * **Code Formatting:**  Use consistent indentation and whitespace to enhance readability.  \\n\\n* **Data Visualization:** Consider integrating a visualization library (e.g., Plotly) for a more interactive experience and better understanding of results.\\n\\n**Example Snippets and Code Improvements:**\\n\\n\\n* **Improved `accent_classification.py`:**\\n    ```python\\n    import warnings\\n    warnings.filterwarnings('ignore')\\n    \\n    def predict_accent(audio_path):\\n        try:\\n            pipe = pipeline(\\\"audio-classification\\\", model=local_model_dir)\\n            # ... (Model inference code here) ... \\n        except Exception as e:\\n            return {\\\"error\\\": str(e)}\\n\\n    @app.post(\\\"/api/v1/accent_classification\\\") \\n    async def predict_accent_classification(data: DataInput):\\n        # ... (Code to load and process the audio)\\n        result = predict_accent(audio_path)\\n        return result\\n    ```\\n\\n\\n**Additional Recommendations:**\\n\\n* **Testing:** Implement unit tests to ensure individual functions are working as expected.  \\n* **Security:**  Review the security of your API (e.g., input validation, authorization, data protection). \\n\\n\\nBy incorporating these suggestions, you can enhance the code's quality, efficiency, and scalability.  Remember that consistent improvement and attention to details are key to a successful project. \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f2e935d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected. Receiving stream...\n",
      "---\n",
      "```python\n",
      "                    import pandas as pd\n",
      "                    \n",
      "                    \n",
      "                    def main():\n",
      "                        \n",
      "                        # Load the data\n",
      "                        data = pd.read_csv(\"titanic.csv\")\n",
      "                        \n",
      "                        # Do something with the data\n",
      "                        # Example: Print the head of the DataFrame\n",
      "                        print(data.head())\n",
      "                        \n",
      "                        \n",
      "                    \n",
      "                    if __name__ == \"__main__\":\n",
      "                        main()\n",
      "                    ```\n",
      "\n",
      "\n",
      "This code creates a simple script that reads in a CSV file called 'titanic.csv', and then prints the first few rows of the DataFrame to the console.\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "1. **Import pandas:** The code starts by importing the `pandas` library. This library is used for data manipulation and analysis.\n",
      "\n",
      "2. **Define a `main()` function:** This function contains the main logic of our script. \n",
      "\n",
      "3. **Read the CSV file:** The script uses the `pd.read_csv()` function to load the data into a Pandas DataFrame named `data`.\n",
      " \n",
      "4. **Print the DataFrame's first few rows:** The code uses `data.head()` to display the first few rows (by default 5).\n",
      "\n",
      "5. **Run the script:**\n",
      "   - The `if __name__ == \"__main__\":` block ensures that the code in the `main()` function is executed only when the script is run directly. \n",
      "\n",
      "**To Use This Script:**\n",
      "\n",
      "1. Save this code as a Python file (e.g., `read_data.py`)\n",
      "2. Make sure you have the Pandas library installed: `pip install pandas` \n",
      "3. Run the script from your terminal using: `python read_data.py`\n",
      "\n",
      "\n",
      "\n",
      "Let me know if you need me to explain any more details about any of this!\n",
      "\n",
      "---\n",
      "Stream finished.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"http://127.0.0.1:8000/model/review\"\n",
    "payload = {\"repo_url\": \"https://github.com/mahmuuudtolba/Titanic-aws\"}\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "# The key is stream=True\n",
    "with requests.post(url, json=payload, headers=headers, stream=True) as response:\n",
    "    if response.ok:\n",
    "        print(\"Successfully connected. Receiving stream...\\n---\")\n",
    "        for chunk in response.iter_content(chunk_size=None, decode_unicode=True):\n",
    "            if chunk:\n",
    "                print(chunk, end='', flush=True)\n",
    "        print(\"\\n---\\nStream finished.\")\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2e5375db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully connected. Receiving stream...\n",
      "---\n",
      "```python\n",
      "                    import pandas as pd\n",
      "                    \n",
      "                    \n",
      "                    def main():\n",
      "                        \n",
      "                        # Load the data\n",
      "                        data = pd.read_csv(\"titanic.csv\")\n",
      "                        \n",
      "                        # Do something with the data\n",
      "                        # Example: Print the head of the DataFrame\n",
      "                        print(data.head())\n",
      "                        \n",
      "                        \n",
      "                    \n",
      "                    if __name__ == \"__main__\":\n",
      "                        main()\n",
      "                    ```\n",
      "\n",
      "\n",
      "This code creates a simple script that reads in a CSV file called 'titanic.csv', and then prints the first few rows of the DataFrame to the console.\n",
      "\n",
      "**Explanation:**\n",
      "\n",
      "1. **Import pandas:** The code starts by importing the `pandas` library. This library is used for data manipulation and analysis.\n",
      "\n",
      "2. **Define a `main()` function:** This function contains the main logic of our script. \n",
      "\n",
      "3. **Read the CSV file:** The script uses the `pd.read_csv()` function to load the data into a Pandas DataFrame named `data`.\n",
      " \n",
      "4. **Print the DataFrame's first few rows:** The code uses `data.head()` to display the first few rows (by default 5).\n",
      "\n",
      "5. **Run the script:**\n",
      "   - The `if __name__ == \"__main__\":` block ensures that the code in the `main()` function is executed only when the script is run directly. \n",
      "\n",
      "**To Use This Script:**\n",
      "\n",
      "1. Save this code as a Python file (e.g., `read_data.py`)\n",
      "2. Make sure you have the Pandas library installed: `pip install pandas` \n",
      "3. Run the script from your terminal using: `python read_data.py`\n",
      "\n",
      "\n",
      "\n",
      "Let me know if you need me to explain any more details about any of this!\n",
      "\n",
      "---\n",
      "Stream finished.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "url = \"http://127.0.0.1:8000/model/review\"\n",
    "payload = {\"repo_url\": \"https://github.com/mahmuuudtolba/Titanic-aws\"}\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "\n",
    "# The key is stream=True\n",
    "with requests.post(url, json=payload, headers=headers, stream=True) as response:\n",
    "    if response.ok:\n",
    "        print(\"Successfully connected. Receiving stream...\\n---\")\n",
    "        for chunk in response.iter_content(chunk_size=None, decode_unicode=True):\n",
    "            if chunk:\n",
    "                print(chunk, end='', flush=True)\n",
    "        print(\"\\n---\\nStream finished.\")\n",
    "    else:\n",
    "        print(f\"Error: {response.status_code}\")\n",
    "        print(response.text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a7a9e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
