
# LLM-Powered GitHub Repository Reviewer


This project is an AI-powered tool that reviews GitHub repositories using a Large Language Model (LLM). It analyzes the codebase of a provided GitHub repository and provides detailed feedback on code quality, potential bugs, and areas for improvement, simulating a review from a senior MLOps engineer.

## How it Works

The application follows these steps to review a repository:
1.  **Check Cache:** Before processing, it checks if a review for the given repository URL already exists in the Redis cache. If found, the cached review is returned immediately.
2.  **Clone Repository:** The user provides a GitHub repository URL, which the application then clones into a temporary directory.
3.  **Extract Content:** It reads the content of all files with specific extensions (e.g., `.py`, `.md`, `.yml`) from the cloned repository.
4.  **Generate Prompt:** The extracted content is then used to construct a detailed prompt for the LLM.
5.  **LLM Review:** The prompt is sent to a pre-loaded Llama C++ model.
6.  **Stream and Cache Response:** The LLM's review is streamed back to the user in real-time. The full review is then saved to the Redis cache for future requests.

## Features

-   **AI-Powered Code Review:** Get intelligent feedback on your code from a powerful LLM.
-   **Efficient Caching:** Uses Redis to cache reviews, providing near-instant responses for previously analyzed repositories.
-   **Real-time Streaming:** Responses from the LLM are streamed, providing a real-time experience.
-   **Easy to Use:** Simply provide a GitHub repository URL to get a review.
-   **Modern Backend:** Built with FastAPI, a modern, high-performance Python web framework.
-   **Flexible and Extensible:** The project is designed with a modular structure, making it easy to extend with new features.

## Performance

The application has been optimized for speed. Initial development versions took approximately **290 seconds** to complete a repository review. After performance improvements, the same process now completes in just **84 seconds**, a **~71%** reduction in processing time.

## Technology Stack

-   **Backend:** FastAPI, Uvicorn
-   **LLM:** Llama C++
-   **Caching:** Redis
-   **Core Libraries:**
    -   `GitPython`: For cloning and interacting with GitHub repositories.
-   **Configuration:** Pydantic

## Project Structure

```
.
├── api/
│   ├── controller/
│   │   ├── DataController.py   # Handles cloning and reading repositories
│   │   └── ModelController.py  # Manages the LLM and prompt generation
│   ├── routes/
│   │   └── model.py            # Defines the API endpoints
│   ├── schemas/
│   │   └── review.py           # Pydantic schemas for request/response
│   ├── utils/
│   │   └── config.py           # Configuration management
│   ├── main.py                 # FastAPI application entry point
│   └── ...
├── download_model.py           # Script to download the LLM model
├── requirements.txt            # Project dependencies
└── ...
```

## Getting Started

### Prerequisites

-   Python 3.11+
-   Pip

### Installation

1.  Clone the repository:

    ```bash
    git clone https://github.com/your-username/LLM-Powered-GitHub-Repository-Reviewer.git
    cd LLM-Powered-GitHub-Repository-Reviewer
    ```

2.  Install the required packages:

    ```bash
    pip install -r requirements.txt
    ```
3. Download the model:
    ```bash
    python download_model.py
    ```

### Running the Application

There are two ways to run the application:

#### 1. Using Docker (Recommended)

This is the easiest way to get started, as it handles all the dependencies and services for you.

1.  Make sure you have Docker and Docker Compose installed.
2.  Run the following command from the root of the project:
    ```bash
    docker-compose up --build
    ```

#### 2. Running Locally

1.  Start a Redis server on `localhost:6379`.
2.  Start the FastAPI server:
    ```bash
    uvicorn api.main:app --reload
    ```
The API will be available at `http://127.0.0.1:8000`.

## API Documentation

The API documentation is automatically generated by FastAPI and is available at `http://127.0.0.1:8000/docs`.

### Endpoints

-   `GET /model/health-check`: Checks if the server is responsive.
-   `POST /model/review`: Reviews a GitHub repository.

#### `POST /model/review`

This endpoint initiates a review of a GitHub repository.

**Request Body:**

```json
{
  "repo_url": "https://github.com/user/repo-name"
}
```

**Response:**

The response is a streaming response of text, where each chunk is a piece of the review generated by the LLM.

**Example Usage with `curl`:**

```bash
curl -X POST "http://127.0.0.1:8000/model/review" -H "Content-Type: application/json" -d '{"repo_url": "https://github.com/fastapi/fastapi"}'
``` 
